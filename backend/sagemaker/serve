#!/usr/bin/env python3
"""
SageMaker serving script
Starts a Flask server that responds to /ping and /invocations
"""
import os
import json
import sys
import traceback
from flask import Flask, request, Response
import inference

app = Flask(__name__)

# Load model on startup
model = None

def load_model():
    """Load the model"""
    global model
    try:
        model_dir = os.environ.get('SM_MODEL_DIR', '/opt/ml/model')
        model = inference.model_fn(model_dir)
        print(f"[serve] Model loaded successfully from {model_dir}", flush=True)
        return model
    except Exception as e:
        print(f"[serve] Error loading model: {e}", flush=True)
        traceback.print_exc()
        return None

@app.route('/ping', methods=['GET'])
def ping():
    """
    Health check endpoint required by SageMaker
    Returns 200 if the model is loaded and ready
    """
    global model
    if model is None:
        model = load_model()
    
    if model is not None:
        print("[serve] Health check: OK", flush=True)
        return Response(response='{"status": "healthy"}', status=200, mimetype='application/json')
    else:
        print("[serve] Health check: FAILED - model not loaded", flush=True)
        return Response(response='{"status": "unhealthy"}', status=503, mimetype='application/json')

@app.route('/invocations', methods=['POST'])
def invoke():
    """
    Inference endpoint required by SageMaker
    Receives JSON input and returns predictions
    """
    global model
    if model is None:
        model = load_model()
        if model is None:
            return Response(
                response=json.dumps({'error': 'Model not loaded'}),
                status=503,
                mimetype='application/json'
            )
    
    try:
        # Get content type
        content_type = request.content_type or 'application/json'
        print(f"[serve] Received request with content-type: {content_type}", flush=True)
        
        # Parse input
        input_data = inference.input_fn(request.data, content_type)
        print(f"[serve] Input parsed successfully", flush=True)
        
        # Run prediction
        prediction = inference.predict_fn(input_data, model)
        print(f"[serve] Prediction completed", flush=True)
        
        # Format output
        output = inference.output_fn(prediction, 'application/json')
        
        return Response(response=output, status=200, mimetype='application/json')
        
    except Exception as e:
        print(f"[serve] Error during inference: {e}", flush=True)
        traceback.print_exc()
        return Response(
            response=json.dumps({'error': str(e)}),
            status=500,
            mimetype='application/json'
        )

if __name__ == '__main__':
    # Load model at startup
    print("[serve] Starting SageMaker inference server...", flush=True)
    model = load_model()
    
    # Start Flask server
    # Use 0.0.0.0 to accept connections from any network interface
    # SageMaker will connect to this
    port = int(os.environ.get('SAGEMAKER_BIND_TO_PORT', 8080))
    print(f"[serve] Starting server on 0.0.0.0:{port}", flush=True)
    
    # Run with Flask's built-in server for simplicity
    # In production, gunicorn would be better but Flask works for now
    app.run(host='0.0.0.0', port=port, debug=False, threaded=True)
